{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Being Clear and Direct\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anthropic in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (0.32.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (0.19.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from tokenizers>=0.13.0->anthropic) (0.24.5)\n",
      "Requirement already satisfied: filelock in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\git\\.py\\claude-prompt-eng\\venv\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "\n",
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import anthropic\n",
    "\n",
    "# Retrieve the API_KEY & MODEL_NAME variables from the IPython store\n",
    "from configSecret import API_KEY\n",
    "from config import MODEL_NAME\n",
    "API_KEY = API_KEY()\n",
    "MODEL_NAME = MODEL_NAME()\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "# Note that we changed max_tokens to 4K just for this lesson to allow for longer completions in the exercises\n",
    "def get_completion(prompt: str, system_prompt=\"\"):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=4000,\n",
    "        temperature=0.0,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "**Claude responds best to clear and direct instructions.**\n",
    "\n",
    "Think of Claude like any other human that is new to the job. **Claude has no context** on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to Claude, the better and more accurate Claude's response will be.\"\t\t\t\t\n",
    "\t\t\t\t\n",
    "When in doubt, follow the **Golden Rule of Clear Prompting**:\n",
    "- Show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the result you want. If they're confused, Claude's confused.\t\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Let's take a task like writing poetry. (Ignore any syllable mismatch - LLMs aren't great at counting syllables yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a haiku about robots:\n",
      "\n",
      "Metallic beings\n",
      "Programmed to serve and obey\n",
      "Artificial life\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This haiku is nice enough, but users may want Claude to go directly into the poem without the \"Here is a haiku\" preamble.\n",
    "\n",
    "How do we achieve that? We **ask for it**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metallic beings,\n",
      "Programmed to serve and obey,\n",
      "Robots, our creations.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots. Skip the preamble; go straight into the poem.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example. Let's ask Claude who's the best basketball player of all time. You can see below that while Claude lists a few names, **it doesn't respond with a definitive \"best\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no universally agreed upon \"best basketball player of all time.\" It's a highly debated topic, as there have been many all-time great players throughout the history of the sport. Some of the players often considered among the greatest of all time include:\n",
      "\n",
      "- Michael Jordan - Widely regarded as one of the greatest, if not the greatest, player ever. He won 6 NBA championships with the Chicago Bulls.\n",
      "\n",
      "- LeBron James - One of the most dominant and well-rounded players of the modern era. He has won 4 NBA titles with 3 different teams.\n",
      "\n",
      "- Kareem Abdul-Jabbar - Legendary center who won 6 NBA championships and holds the record for most career points scored.\n",
      "\n",
      "- Wilt Chamberlain - Dominant big man who holds numerous NBA records including most points in a game (100).\n",
      "\n",
      "- Magic Johnson - Pioneering point guard who led the \"Showtime\" Lakers to 5 NBA titles.\n",
      "\n",
      "- Bill Russell - Defensive anchor and 11-time NBA champion with the Boston Celtics.\n",
      "\n",
      "There are good arguments for several players as the \"GOAT\" (greatest of all time), but it ultimately comes down to personal opinion and how one weighs different accomplishments and skills. The debate continues among basketball fans and experts.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Who is the best basketball player of all time?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get Claude to make up its mind and decide on a best player? Yes! Just ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no definitive consensus on who the single \"best\" basketball player of all time is, as it is a highly subjective and debatable topic. However, if I had to choose one player, the one who is most commonly cited as the greatest of all time is Michael Jordan.\n",
      "\n",
      "Some key reasons why Michael Jordan is often considered the GOAT (Greatest of All Time) in basketball:\n",
      "\n",
      "- Dominant statistical career - Jordan is one of the most prolific scorers in NBA history, averaging over 30 points per game for his career. He also excelled in other statistical categories.\n",
      "\n",
      "- Unparalleled individual accolades - Jordan won 5 MVP awards, 6 NBA championships, 6 NBA Finals MVPs, and 10 scoring titles among many other honors.\n",
      "\n",
      "- Transformative impact on the game - Jordan's style of play, competitiveness, and global popularity helped take the NBA to new heights in popularity and influence.\n",
      "\n",
      "- Sustained excellence over his prime years - Jordan was at the peak of his powers for over a decade, dominating the league during his time.\n",
      "\n",
      "However, there are other all-time greats like LeBron James, Kareem Abdul-Jabbar, Wilt Chamberlain, and others who also have strong cases as the GOAT. But Michael Jordan is the player most commonly cited as the best basketball player of all time.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the [**Example Playground**](#example-playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "- [Exercise 2.1 - Spanish](#exercise-21---spanish)\n",
    "- [Exercise 2.2 - One Player Only](#exercise-22---one-player-only)\n",
    "- [Exercise 2.3 - Write a Story](#exercise-23---write-a-story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 - Spanish\n",
    "Modify the `SYSTEM_PROMPT` to make Claude output its answer in Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Szia! Én jól vagyok, köszönöm, hogy megkérdezted. Örülök, hogy találkoztunk. Hogy vagy te?\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# System prompt - this is the only field you should chnage\n",
    "SYSTEM_PROMPT = \"Válaszolj magyarul\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Hello Claude, how are you?\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return \"szia\" in text.lower()\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_2_1_hint; print(exercise_2_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 - One Player Only\n",
    "\n",
    "Modify the `PROMPT` so that Claude doesn't equivocate at all and responds with **ONLY** the name of one specific player, with **no other words or punctuation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Jordan.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# Prompt - this is the only field you should change\n",
    "PROMPT = \"who is the GOAT basketball player. Answer with only one name\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return text == \"Michael Jordan.\"\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_2_2_hint; print(exercise_2_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 - Write a Story\n",
    "\n",
    "Modify the `PROMPT` so that Claude responds with as long a response as you can muster. If your answer is **over 800 words**, Claude's response will be graded as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt - this is the only field you should change\n",
    "PROMPT = \"[Replace this text]\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    trimmed = text.strip()\n",
    "    words = len(trimmed.split())\n",
    "    return words >= 800\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hints import exercise_2_3_hint; print(exercise_2_3_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Playground\n",
    "\n",
    "This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Write a haiku about robots. Skip the preamble; go straight into the poem.\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Who is the best basketball player of all time?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Who is the best basketball player of all time? Yes, there are differing opinions, but if you absolutely had to pick one player, who would it be?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
